{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import os\n",
    "import yaml\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "\n",
    "import pneumapackage.iodata as rd\n",
    "import pneumapackage.mapmatching as mm\n",
    "from pneumapackage.__init__ import write_pickle, path_data\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# if we want to create a small dataset\n",
    "newdata_generation = False\n",
    "num_vechicles = 100\n",
    "smallgen_file = '20181024_d1_0930_1000.csv'\n",
    "\n",
    "# if we want to create train and test datasets\n",
    "traintest_generation = False\n",
    "test_size = 90\n",
    "traintest_file = '20181024_d1_1000_1030.csv'\n",
    "\n",
    "# if we want to create a new smaller map csv\n",
    "smallmap_generation = False\n",
    "extend_map = True\n",
    "smallmap_file = 'drivable_map_d1_ex.csv'\n",
    "\n",
    "# if we want to use full, small or test dataset\n",
    "small_dataset = False\n",
    "test_dataset = True\n",
    "file_group = '1024_d1_1000' # day_DSid_starttime of file we use\n",
    "\n",
    "# if we want to create a csv without motorcycles\n",
    "no_motorcycle_generation = False\n",
    "nomotor_file = '20181024_d1_0900_0930.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "Create a smaller dataset in the directory data/trajectory_small, with less vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataset in data_pneumatrajectory_small, with first 'num_vechicles'\n",
    "def gen_data():\n",
    "\n",
    "    data_dir = 'data_pneuma/trajectory'\n",
    "    data_dir = os.path.join(data_dir, smallgen_file) # Current full DS file\n",
    "    \n",
    "    newdata_dir = 'data_pneuma/trajectory_small'\n",
    "    newdata_dir = os.path.join(newdata_dir, smallgen_file) # New small DS file\n",
    "    \n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    # Get list from full DS\n",
    "    with open(data_dir, 'r') as data_obj:\n",
    "        data_read = csv.reader(data_obj)\n",
    "        data = list(data_read)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    # Create list with first num_vehicles\n",
    "    for row in range(num_vechicles):\n",
    "        new_data.append(data[row][:])\n",
    "        \n",
    "    # Create new csv file\n",
    "    with open(newdata_dir, 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerows(new_data)\n",
    "\n",
    "\n",
    "if newdata_generation:\n",
    "    gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one testing dataset with 'test_size' random vehicles\n",
    "# and one training dataset with tot - 'test_size' vehicles\n",
    "def gen_traintest_data():\n",
    "    \n",
    "    data_dir = 'data_pneuma/trajectory'\n",
    "    data_dir = os.path.join(data_dir, traintest_file)\n",
    "    \n",
    "    train_dir = 'data_pneuma/trajectory_train'\n",
    "    train_dir = os.path.join(train_dir, traintest_file)\n",
    "    \n",
    "    test_dir = 'data_pneuma/trajectory_test'\n",
    "    test_dir = os.path.join(test_dir, traintest_file)\n",
    "    \n",
    "    \n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    # Get list from full DS\n",
    "    with open(data_dir, 'r') as data_obj:\n",
    "        data_read = csv.reader(data_obj)\n",
    "        data = np.array(list(data_read))\n",
    "    \n",
    "    # Create random mask with 1 where test vehicles are\n",
    "    test_mask = np.zeros(len(data), bool)\n",
    "    test_indices = np.random.choice(np.arange(1, len(data)), test_size, replace=False)\n",
    "    test_mask[0] = 1 # always take first row (columns names)\n",
    "    test_mask[test_indices] = 1\n",
    "    \n",
    "    # create list with test data\n",
    "    test_data = data[test_mask][:]\n",
    "    \n",
    "    #create list with train data\n",
    "    test_mask[0] = 0\n",
    "    train_data = data[~test_mask][:]\n",
    "    \n",
    "    # Create new train csv file\n",
    "    with open(train_dir, 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerows(train_data)\n",
    "        \n",
    "    # Create new test csv file\n",
    "    with open(test_dir, 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerows(test_data)\n",
    "    \n",
    "        \n",
    "if traintest_generation:\n",
    "    gen_traintest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file same as the original but without motorcycles\n",
    "def no_motorcyle_csv():\n",
    "    data_dir = 'data_pneuma/trajectory'\n",
    "    data_dir = os.path.join(data_dir, nomotor_file) # Current full DS file\n",
    "    \n",
    "    newdata_dir = 'data_pneuma/trajectory_nomotor'\n",
    "    newdata_dir = os.path.join(newdata_dir, nomotor_file) # New DS file\n",
    "    \n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    # Get list from full DS\n",
    "    with open(data_dir, 'r') as data_obj:\n",
    "        data_read = csv.reader(data_obj, delimiter=';')\n",
    "        data = list(data_read)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    # Create list with all vehicles except motorcycles\n",
    "    for row in range(1, len(data)):\n",
    "        if data[row][1] != ' Motorcycle':\n",
    "            new_data.append(data[row][:])\n",
    "        \n",
    "    # Create new csv file\n",
    "    with open(newdata_dir, 'w') as f:\n",
    "        write = csv.writer(f, delimiter=';')\n",
    "        write.writerow(data[0][:]) #create header\n",
    "        write.writerows(new_data)\n",
    "    \n",
    "\n",
    "if no_motorcycle_generation:\n",
    "    no_motorcyle_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories creation\n",
    "Create the hdf5 file with adjusted trajectories from the dataset, which will then will be used to load the trajectories for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main: store data in 'pneuma_hdf' in HDF5 format,\n",
    "# store path to hdf data, group names (date, zone, time), etc. in 'path_dict'\n",
    "# both in folder 'data'\n",
    "# select folder where data is, based on what we want to do\n",
    "def all_data_to_hdf():\n",
    "    \n",
    "    # init data dictionary and create file 'datasets_paths' in folder 'data'\n",
    "    path = 'data_pneuma/trajectory/' # path to raw data\n",
    "    if small_dataset:\n",
    "        path = 'data_pneuma/trajectory_small/' # path to raw data\n",
    "    if test_dataset:\n",
    "        path = 'data_pneuma/trajectory_test/' # path to raw data\n",
    "    data_dict = rd.initialize_data_paths(path) #create a dict with all data groups in the folder (id: path)\n",
    "    \n",
    "    for k in data_dict.keys(): #for every group in the folder\n",
    "        _ = get_hdf_names(k) #store data in HDF5 format\n",
    "\n",
    "    print('All data stored in HDF5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def get_hdf_names(group_id):\n",
    "    try:\n",
    "        # if data is already stored in HDF5, simply return group_path\n",
    "        group_path = rd.get_group(group_id) \n",
    "    except KeyError:\n",
    "        # otherwise, store data in HDF5 format \n",
    "        _, group_path = rd.io_data(group_id)\n",
    "        \n",
    "    path_id = group_path + '/all_id'\n",
    "    path_org = group_path + '/original_trajectories'\n",
    "    path_adj = group_path + '/adjusted_trajectories'\n",
    "    \n",
    "    return path_id, path_org, path_adj\n",
    "\n",
    "\n",
    "all_data_to_hdf()\n",
    "\n",
    "\n",
    "# main: adjust raw trajectory data by group, store in hdf\n",
    "def adjust_data_in_hdf():\n",
    "    tic = time.time()\n",
    "    groups = rd.get_path_dict()['groups'].keys() #get the IDs of the stored data groups\n",
    "    for i in groups:\n",
    "        print(f'Dataset: {i}')\n",
    "        _ = adjust_traj(i)\n",
    "    toc = time.time()\n",
    "    print(f'All datasets adjusted, took {toc-tic} sec')\n",
    "    \n",
    "# helper: adjust trajectory by group (make all timestamps the same)\n",
    "def adjust_traj(group_id, redo=False, bearing=True, resample=True, step=1000):\n",
    "    tic = time.time()\n",
    "    print('Start: â€¦load trajectories (resampled)')\n",
    "    hdf_path = rd.get_hdf_path()\n",
    "    path_id, path_org, path_adj = get_hdf_names(group_id)\n",
    "    if redo:\n",
    "        print('Create adjusted trajectory table and save the file in HDF5')\n",
    "        ldf = rd.get_from_hdf(hdf_path, path_id, path_org, result='list')\n",
    "        rd.new_dfs(ldf, group_id, bearing=bearing, resample=resample, step=step)\n",
    "        ldf = rd.get_from_hdf(hdf_path, path_id, path_adj, result='list')\n",
    "    else:\n",
    "        try:\n",
    "            ldf = rd.get_from_hdf(hdf_path, path_id, path_adj, result='list') #if we already have adj traj\n",
    "        except KeyError:\n",
    "            print('Create adjusted trajectory table and save the file in HDF5')\n",
    "            ldf = rd.get_from_hdf(hdf_path, path_id, path_org, result='list')\n",
    "            rd.new_dfs(ldf, group_id, bearing=bearing, resample=resample, step=step)\n",
    "            ldf = rd.get_from_hdf(hdf_path, path_id, path_adj, result='list')\n",
    "    toc = time.time()\n",
    "    print(f'Adjusted trajectories loaded, took {toc - tic}')\n",
    "    return ldf\n",
    "\n",
    "\n",
    "adjust_data_in_hdf()\n",
    "\n",
    "# main: match trajectory to Athens map, store in hdf\n",
    "# - generate map files \"athensmap.dat' and 'athensmap.idx'\n",
    "def match_all_data_in_hdf():\n",
    "    tic = time.time()\n",
    "    groups = rd.get_path_dict()['groups'].keys()\n",
    "    for i in groups:\n",
    "        print(f'Dataset: {i}')\n",
    "        _ = match_line_trajectories(i)\n",
    "    toc = time.time()\n",
    "    print(f'All datasets matched, took {toc-tic} sec')\n",
    "\n",
    "\n",
    "# helper: if trajectories have already been matched, simply return matched data. otherwise, do map-matching\n",
    "def match_line_trajectories(group_id, selection=None, reload=False, **kwargs):\n",
    "    tic = time.time()\n",
    "    print('Start: â€¦load matched trajectories')\n",
    "    hdf_path = rd.get_hdf_path()\n",
    "    group_path = rd.get_path_dict()['groups'][group_id]\n",
    "    if reload:\n",
    "        traj_obj = map_matching(group_id=group_id, **kwargs)\n",
    "        traj_matched = traj_obj['tracks']\n",
    "        network_obj = traj_obj['network']\n",
    "        traj_match = mm.TransformTrajectories(traj_matched, network_obj)\n",
    "        step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "        traj_match.tracks_line.to_hdf(hdf_path, key=group_path + f'/mm_line_{step}ms', format='table', mode='a',\n",
    "                                      append=False,\n",
    "                                      data_columns=['time', 'u_match', 'v_match'])\n",
    "        line_traj = traj_match.tracks_line\n",
    "        if selection is not None:\n",
    "            line_traj = traj_match.tracks_line.query(selection)\n",
    "    else:\n",
    "        try:\n",
    "            step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "            line_traj = rd.get_from_hdf(hdf_path, key_id=group_path + '/all_id', key_tr=group_path +\n",
    "                                                                                        f'/mm_line_{step}ms',\n",
    "                                        result='df_all', select_tr=selection)\n",
    "        except (KeyError, TypeError):\n",
    "            traj_obj = map_matching(group_id=group_id, **kwargs)\n",
    "            traj_matched = traj_obj['tracks']\n",
    "            network_obj = traj_obj['network']\n",
    "            traj_match = mm.TransformTrajectories(traj_matched, network_obj)\n",
    "            step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "            traj_match.tracks_line.to_hdf(hdf_path, key=group_path + f'/mm_line_{step}ms', format='table', mode='a',\n",
    "                                          append=False, data_columns=['time', 'u_match', 'v_match'])\n",
    "            line_traj = traj_match.tracks_line\n",
    "            if selection is not None:\n",
    "                line_traj = traj_match.tracks_line.query(selection)\n",
    "\n",
    "    toc = time.time()\n",
    "    print(f'Matched trajectories loaded, took {toc - tic} sec')\n",
    "    return line_traj\n",
    "\n",
    "# helper: do map-matching by group\n",
    "def map_matching(group_id, traj_obj='/adjusted_trajectories', max_distance=10, rematch=False,\n",
    "                      match_latlon=True, save_shp=False, path=path_data):\n",
    "    tic = time.time()\n",
    "    print('Start: â€¦load map matching')\n",
    "    # load network\n",
    "    network_obj = load_network() # function defined in this notebook\n",
    "    hdf_path = rd.get_hdf_path()\n",
    "    group_path = rd.get_path_dict()['groups'][group_id]\n",
    "    step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "    if traj_obj not in ['/original_trajectories', '/adjusted_trajectories']:\n",
    "        raise ValueError(f'traj_obj should be in [\"/original_trajectories\", \"/adjusted_trajectories\"]')\n",
    "    try:\n",
    "        with h5py.File(hdf_path, 'r') as s:\n",
    "            tag_match = s[group_path].attrs[f'tag_mapmatching_{step}']\n",
    "        if tag_match != network_obj.mm_id[group_id]:\n",
    "            rematch = True\n",
    "    except KeyError:\n",
    "        rematch = True\n",
    "\n",
    "    if not rematch:\n",
    "        try:\n",
    "            dfmatch_all = rd.get_from_hdf(hdf_path, key_id=group_path + '/all_id', key_tr=group_path + f'/mm_{step}ms',\n",
    "                                          result='df_all')\n",
    "        except (KeyError, TypeError):\n",
    "            max_init_dist = int(max(network_obj.network_edges['length'])) + 1\n",
    "            print(f'Initial distance: {max_init_dist} m, maximum distance (start): {max_distance} m')\n",
    "            traj_unm = rd.get_from_hdf(hdf_path, key_id=group_path + '/all_id', key_tr=group_path + traj_obj,\n",
    "                                       result='list')\n",
    "            tmm = mm.MapMatching(traj_unm, network_obj, max_init=max_init_dist, max_d=max_distance,\n",
    "                                 match_latlon=match_latlon)\n",
    "            match_all = tmm.match_variable_distance(progress=False)\n",
    "            dfmatch_all = pd.concat(match_all)\n",
    "            step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "            dfmatch_all.to_hdf(hdf_path, key=group_path + f'/mm_{step}ms', format='table', mode='a', append=False,\n",
    "                               data_columns=['track_id', 'time', 'n1', 'n2', '_id'])\n",
    "            dt = datetime.datetime.now()\n",
    "            tag = int(dt.strftime('%Y%m%d%H%M'))\n",
    "            with h5py.File(hdf_path, 'a') as s:\n",
    "                s[group_path].attrs[f'tag_mapmatching_{step}'] = tag\n",
    "            network_obj.add_mapmatch_tag(group_id, tag)\n",
    "            write_pickle(network_obj, 'network', path)\n",
    "    else:\n",
    "        max_init_dist = int(max(network_obj.network_edges['length'])) + 1\n",
    "        print(f'Initial distance: {max_init_dist} m, maximum distance (start): {max_distance} m')\n",
    "        step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "        traj_unm = rd.get_from_hdf(hdf_path, key_id=group_path + '/all_id', key_tr=group_path + traj_obj,\n",
    "                                   result='list')\n",
    "        tmm = mm.MapMatching(traj_unm, network_obj, max_init=max_init_dist, max_d=max_distance,\n",
    "                             match_latlon=match_latlon)\n",
    "        match_all = tmm.match_variable_distance(progress=False)\n",
    "        dfmatch_all = pd.concat(match_all)\n",
    "        step = rd.get_path_dict()['current_resample_step'][group_id]\n",
    "        dfmatch_all.to_hdf(hdf_path, key=group_path + f'/mm_{step}ms', format='table', mode='a', append=False,\n",
    "                           data_columns=['track_id', 'time', 'n1', 'n2', '_id'])\n",
    "        dt = datetime.datetime.now()\n",
    "        tag = int(dt.strftime('%Y%m%d%H%M'))\n",
    "        with h5py.File(hdf_path, 'a') as s:\n",
    "            s[group_path].attrs[f'tag_mapmatching_{step}'] = tag\n",
    "        network_obj.add_mapmatch_tag(group_id, tag)\n",
    "        write_pickle(network_obj, 'network', path)\n",
    "\n",
    "    if save_shp:\n",
    "        fn = path + f'/shapefiles/used_network_{group_id}_mm{step}'\n",
    "        used_network = network_obj.network_edges[network_obj.network_edges['_id'].isin(dfmatch_all['_id'])]\n",
    "        used_network.loc[:, ~used_network.columns.isin(['edge'])].to_file(filename=fn)\n",
    "        Path(path + \"/used_network\").mkdir(parents=True, exist_ok=True)\n",
    "        write_pickle(used_network, f'used_network_{group_id}_mm{step}', path=path + '/used_network')\n",
    "        print('Shapefiles stored')\n",
    "\n",
    "    toc = time.time()\n",
    "    print(f'Map-matched trajectories loaded, took {toc - tic} sec')\n",
    "    return {'tracks': dfmatch_all, 'network': network_obj}\n",
    "\n",
    "# load network\n",
    "def load_network(plot=False, save_to_shp=False):\n",
    "    path = 'data_pneuma/'\n",
    "    filename = 'network'\n",
    "    with open(path + filename, 'rb') as a: #load the map pickle file\n",
    "        net = pickle.load(a)\n",
    "    \n",
    "    if plot:\n",
    "        net.plot_network_lanes()  # plot network\n",
    "    \n",
    "    if save_to_shp:\n",
    "        net.save_graph_to_shp() # save to shapefiles in folder 'data'\n",
    "        \n",
    "    return net\n",
    "\n",
    "match_all_data_in_hdf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small map generation\n",
    "Create the csv file with the smaller map of current dataset (e.g. area 1 of pneuma dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_small_map():\n",
    "    \n",
    "    hdf_path = rd.get_hdf_path() #get path to HDF file with all data (pneuma_hdf)\n",
    "    group_path = rd.get_path_dict()['groups'][file_group]\n",
    "    traj_obj = '/mm_line_1000ms'\n",
    "    \n",
    "    map_dir = 'data_pneuma/map' # map directory\n",
    "    map_file = 'drivable_map.csv' # map file name\n",
    "    \n",
    "    fullmap_dir = os.path.join(map_dir, map_file) # Current full map file\n",
    "    smallmap_dir = os.path.join(map_dir, smallmap_file) # New small map file\n",
    "    \n",
    "    # get trajectories data from dataset we are using\n",
    "    _, traj_df, _ = rd.get_from_hdf(hdf_path, key_id=group_path + '/all_id',\n",
    "                                            key_tr=group_path + traj_obj, result='all')\n",
    "    \n",
    "    # get the min and max longitued and latitude between all traj\n",
    "    min_lon = min(np.array(traj_df['lon_1']))\n",
    "    max_lon = max(np.array(traj_df['lon_1']))\n",
    "    min_lat = min(np.array(traj_df['lat_1']))\n",
    "    max_lat = max(np.array(traj_df['lat_1']))\n",
    "    \n",
    "    \n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    map_df = pd.read_csv(fullmap_dir)\n",
    "    \n",
    "    # get the min map that includes all trajectories\n",
    "    map_reduced = map_df[(map_df.lon >= min_lon) & (map_df.lon <= max_lon) &\n",
    "                         (map_df.lat >= min_lat) & (map_df.lat <= max_lat)]\n",
    "    \n",
    "    # used for numerical map dimensions reasons, to be used later in the network (same rows and cols)\n",
    "    if extend_map:\n",
    "        min_x = min(np.array(map_reduced['idx_x']))\n",
    "        max_x = max(np.array(map_reduced['idx_x']))\n",
    "        min_y = min(np.array(map_reduced['idx_y']))\n",
    "        max_y = max(np.array(map_reduced['idx_y']))\n",
    "        \n",
    "        map_reduced = map_df[(map_df.idx_x >= min_x) & (map_df.idx_x <= max_x+7) &\n",
    "                             (map_df.idx_y >= min_y) & (map_df.idx_y <= max_y)]\n",
    "    \n",
    "    # write new csv file with small map\n",
    "    map_reduced.to_csv(smallmap_dir)\n",
    "    \n",
    "if smallmap_generation:\n",
    "    create_small_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters loading\n",
    "\n",
    "Load all the parameters from parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='STAR')\n",
    "    parser.add_argument('--dataset', default='eth5')\n",
    "    parser.add_argument('--save_dir')\n",
    "    parser.add_argument('--model_dir')\n",
    "    parser.add_argument('--config')\n",
    "    parser.add_argument('--using_cuda', default=False, type=ast.literal_eval)\n",
    "    parser.add_argument('--test_set', default='eth', type=str, help='Set this value to [eth, hotel, zara1, zara2, univ] for ETH-univ, ETH-hotel, UCY-zara01, UCY-zara02, UCY-univ')\n",
    "    parser.add_argument('--base_dir', default='.', help='Base directory including these scripts.')\n",
    "    parser.add_argument('--save_base_dir', default='./output/', help='Directory for saving caches and models.')\n",
    "    parser.add_argument('--phase', default='train', help='Set this value to \\'train\\' or \\'test\\'')\n",
    "    parser.add_argument('--train_model', default='star', help='Your model name')\n",
    "    parser.add_argument('--load_model', default=None, type=str, help=\"load pretrained model for test or training\")\n",
    "    parser.add_argument('--model', default='star.STAR')\n",
    "    parser.add_argument('--seq_length', default=10, type=int)\n",
    "    parser.add_argument('--obs_length', default=4, type=int)\n",
    "    parser.add_argument('--pred_length', default=6, type=int)\n",
    "    parser.add_argument('--batch_around_ped', default=128, type=int)\n",
    "    parser.add_argument('--batch_size', default=8, type=int)\n",
    "    parser.add_argument('--test_batch_size', default=4, type=int)\n",
    "    parser.add_argument('--show_step', default=100, type=int)\n",
    "    parser.add_argument('--start_test', default=10, type=int)\n",
    "    parser.add_argument('--sample_num', default=20, type=int)\n",
    "    parser.add_argument('--num_epochs', default=3, type=int)\n",
    "    parser.add_argument('--ifshow_detail', default=True, type=ast.literal_eval)\n",
    "    parser.add_argument('--ifsave_results', default=False, type=ast.literal_eval)\n",
    "    parser.add_argument('--randomRotate', default=False, type=ast.literal_eval, help=\"=True:random rotation of each trajectory fragment\")\n",
    "    parser.add_argument('--neighbor_thred', default=100, type=int)\n",
    "    parser.add_argument('--learning_rate', default=0.0015, type=float)\n",
    "    parser.add_argument('--clip', default=1, type=int)\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "parser = get_parser()\n",
    "p = parser.parse_args(args=[])\n",
    "\n",
    "p.save_dir = p.save_base_dir + str(p.test_set) + '/'\n",
    "p.model_dir = p.save_base_dir + str(p.test_set) + '/' + p.train_model + '/'\n",
    "p.config = p.model_dir + '/config_' + p.phase + '.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arg(p):\n",
    "    # save arg\n",
    "    if os.path.exists(p.config):\n",
    "        with open(p.config, 'r') as f:\n",
    "            default_arg = yaml.load(f)\n",
    "        key = vars(p).keys()\n",
    "        for k in default_arg.keys():\n",
    "            if k not in key:\n",
    "                print('WRONG ARG: {}'.format(k))\n",
    "                try:\n",
    "                    assert (k in key)\n",
    "                except:\n",
    "                    s = 1\n",
    "        parser.set_defaults(**default_arg)\n",
    "        return parser.parse_args(args=[])\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def save_arg(args):\n",
    "    # save arg\n",
    "    arg_dict = vars(args)\n",
    "    if not os.path.exists(args.model_dir):\n",
    "        os.makedirs(args.model_dir)\n",
    "    with open(args.config, 'w') as f:\n",
    "        yaml.dump(arg_dict, f)\n",
    "        \n",
    "\n",
    "if not load_arg(p):\n",
    "    save_arg(p)\n",
    "\n",
    "args = load_arg(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network input processing\n",
    "\n",
    "Define classes and functions to process the input of our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processor(object):\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.dataloader = Trajectory_Dataloader(args)\n",
    "\n",
    "        if not os.path.isdir(self.args.model_dir):\n",
    "            os.mkdir(self.args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory_Dataloader():\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.args = args\n",
    "        if self.args.dataset == 'eth5':\n",
    "\n",
    "            #self.data_dirs = ['data_pneuma/trajectory_small/']\n",
    "            self.data_dirs = ['data_pneuma/trajectory/']\n",
    "\n",
    "            # Data directory where the pre-processed pickle file resides\n",
    "            self.data_dir = 'data'\n",
    "            skip = 1\n",
    "\n",
    "            self.train_dir = self.data_dirs\n",
    "            self.trainskip = skip\n",
    "            self.test_dir = self.data_dirs\n",
    "            self.testskip = skip\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.train_data_file = os.path.join(self.args.save_dir, \"train_trajectories.cpkl\")\n",
    "        self.test_data_file = os.path.join(self.args.save_dir, \"test_trajectories.cpkl\")\n",
    "        self.train_batch_cache = os.path.join(self.args.save_dir, \"train_batch_cache.cpkl\")\n",
    "        self.test_batch_cache = os.path.join(self.args.save_dir, \"test_batch_cache.cpkl\")\n",
    "\n",
    "        print(\"Creating pre-processed data from raw data.\")\n",
    "        if not test_dataset:\n",
    "            self.traject_preprocess('train')\n",
    "        self.traject_preprocess('test')\n",
    "        print(\"Done.\")\n",
    "\n",
    "        # Load the processed data from the pickle file\n",
    "        print(\"Preparing data batches.\")\n",
    "        \n",
    "        # if we are using test dataset, skip train batches computation\n",
    "        if not (os.path.exists(self.train_batch_cache)) and not test_dataset:\n",
    "            self.frameped_dict, self.pedtraject_dict = self.load_dict(self.train_data_file)\n",
    "            self.dataPreprocess('train')\n",
    "            \n",
    "            \n",
    "        if not (os.path.exists(self.test_batch_cache)):\n",
    "            self.test_frameped_dict, self.test_pedtraject_dict = self.load_dict(self.test_data_file)\n",
    "            self.dataPreprocess('test')\n",
    "\n",
    "        if not test_dataset:\n",
    "            self.trainbatch, self.trainbatchnums, _, _ = self.load_cache(self.train_batch_cache) #!!\n",
    "        self.testbatch, self.testbatchnums, _, _ = self.load_cache(self.test_batch_cache)\n",
    "        print(\"Done.\")\n",
    "\n",
    "        if not test_dataset:\n",
    "            print('Total number of training batches:', self.trainbatchnums)\n",
    "        print('Total number of test batches:', self.testbatchnums)\n",
    "\n",
    "    \n",
    "    def load_cache(self, data_file):\n",
    "        f = open(data_file, 'rb')\n",
    "        raw_data = pickle.load(f)\n",
    "        f.close()\n",
    "        return raw_data\n",
    "    \n",
    "    def load_dict(self, data_file):\n",
    "        f = open(data_file, 'rb')\n",
    "        raw_data = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        frameped_dict = raw_data[0]\n",
    "        pedtraject_dict = raw_data[1]\n",
    "\n",
    "        return frameped_dict, pedtraject_dict\n",
    "    \n",
    "    \n",
    "    # MODIFIED FUNCTION\n",
    "    def traject_preprocess(self, setname):\n",
    "        '''\n",
    "        data_dirs : List of directories where raw data resides\n",
    "        data_file : The file into which all the pre-processed data needs to be stored\n",
    "        '''\n",
    "        if setname == 'train':\n",
    "            data_dirs = self.train_dir\n",
    "            data_file = self.train_data_file\n",
    "        else:\n",
    "            data_dirs = self.test_dir\n",
    "            data_file = self.test_data_file\n",
    "        \n",
    "        # vehicle's traj dataframe column number where timestamps, x, y are\n",
    "        time_col = 19\n",
    "        x_col = 6\n",
    "        y_col = 7\n",
    "        \n",
    "        numFrame_data = []\n",
    "\n",
    "        Pedlist_data = []\n",
    "        frameped_dict = []\n",
    "        pedtrajec_dict = []\n",
    "        \n",
    "        for seti, _ in enumerate(data_dirs):\n",
    "            \n",
    "            hdf_path = rd.get_hdf_path() #get path to HDF file with all data (pneuma_hdf)\n",
    "            group_path = rd.get_path_dict()['groups'][file_group]\n",
    "            traj_obj = '/mm_line_1000ms'  # matched trajectories\n",
    "            \n",
    "            # id_df = all ids of peds in the dataset\n",
    "            # traj_list = list of list, one list with traj data for every ped\n",
    "            id_df, _, traj_list = rd.get_from_hdf(hdf_path, key_id=group_path + '/all_id',\n",
    "                                                  key_tr=group_path + traj_obj, result='all')\n",
    "            \n",
    "            Pedlist = np.arange(1, len(traj_list)+1)\n",
    "            numPeds = len(Pedlist)\n",
    "            # Add the list of frameIDs to the frameList_data\n",
    "            Pedlist_data.append(Pedlist)\n",
    "\n",
    "            numFrame_data.append([])\n",
    "            frameped_dict.append({})\n",
    "            pedtrajec_dict.append({})\n",
    "            \n",
    "            for ind, pedi in enumerate(Pedlist):\n",
    "                if ind % 100 == 0:\n",
    "                    print(ind, len(Pedlist))\n",
    "                \n",
    "                Dataped = np.array(traj_list[ind]) # array with traj data of ped\n",
    "                FrameList = np.around(Dataped[:, time_col]/1000) #list of traj frames (from ms to s)\n",
    "                \n",
    "                if len(FrameList) < 2:\n",
    "                    continue\n",
    "                # Add number of frames of this trajectory\n",
    "                numFrame_data[seti].append(len(FrameList))\n",
    "                # Initialize the row of the numpy array\n",
    "                Trajectories = []\n",
    "                # For each frame for the current ped\n",
    "                \n",
    "                for fi, frame in enumerate(FrameList):\n",
    "                    # Extract the x and y positions\n",
    "                    current_x = Dataped[fi, x_col]\n",
    "                    current_y = Dataped[fi, y_col]\n",
    "                    # Add the pedID, x, y to the row of the numpy array\n",
    "                    Trajectories.append([int(frame), current_x, current_y])\n",
    "                    if int(frame) not in frameped_dict[seti]:\n",
    "                        frameped_dict[seti][int(frame)] = []\n",
    "                    frameped_dict[seti][int(frame)].append(pedi)\n",
    "                pedtrajec_dict[seti][pedi] = np.array(Trajectories)\n",
    "        \n",
    "        f = open(data_file, \"wb\")\n",
    "        pickle.dump((frameped_dict, pedtrajec_dict), f, protocol=2)\n",
    "        f.close()\n",
    "    \n",
    "    \n",
    "    def dataPreprocess(self, setname):\n",
    "        '''\n",
    "        Function to load the pre-processed data into the DataLoader object\n",
    "        '''\n",
    "        if setname == 'train':\n",
    "            val_fraction = 0\n",
    "            frameped_dict = self.frameped_dict\n",
    "            pedtraject_dict = self.pedtraject_dict\n",
    "            cachefile = self.train_batch_cache\n",
    "\n",
    "        else:\n",
    "            val_fraction = 0\n",
    "            frameped_dict = self.test_frameped_dict\n",
    "            pedtraject_dict = self.test_pedtraject_dict\n",
    "            cachefile = self.test_batch_cache\n",
    "        if setname != 'train':\n",
    "            shuffle = False\n",
    "        else:\n",
    "            shuffle = True\n",
    "        \n",
    "        data_index = self.get_data_index(frameped_dict, setname, ifshuffle=shuffle)\n",
    "        \n",
    "        val_index = data_index[:, :int(data_index.shape[1] * val_fraction)]\n",
    "        train_index = data_index[:, (int(data_index.shape[1] * val_fraction) + 1):]\n",
    "        \n",
    "        \n",
    "        trainbatch = self.get_seq_from_index_balance(frameped_dict, pedtraject_dict, train_index, setname)\n",
    "        valbatch = self.get_seq_from_index_balance(frameped_dict, pedtraject_dict, val_index, setname)\n",
    "        \n",
    "        trainbatchnums = len(trainbatch)\n",
    "        valbatchnums = len(valbatch)\n",
    "\n",
    "        f = open(cachefile, \"wb\")\n",
    "        pickle.dump((trainbatch, trainbatchnums, valbatch, valbatchnums), f, protocol=2)\n",
    "        f.close()\n",
    "        \n",
    "    def get_data_index(self, data_dict, setname, ifshuffle=True):\n",
    "        '''\n",
    "        Get the dataset sampling index.\n",
    "        '''\n",
    "        set_id = []\n",
    "        frame_id_in_set = []\n",
    "        total_frame = 0\n",
    "        for seti, dict in enumerate(data_dict):\n",
    "            frames = sorted(dict)\n",
    "            maxframe = max(frames) - self.args.seq_length\n",
    "            frames = [x for x in frames if not x > maxframe]\n",
    "            total_frame += len(frames)\n",
    "            set_id.extend(list(seti for i in range(len(frames))))\n",
    "            frame_id_in_set.extend(list(frames[i] for i in range(len(frames))))\n",
    "\n",
    "        all_frame_id_list = list(i for i in range(total_frame))\n",
    "\n",
    "        data_index = np.concatenate((np.array([frame_id_in_set], dtype=int), np.array([set_id], dtype=int),\n",
    "                                     np.array([all_frame_id_list], dtype=int)), 0)\n",
    "        \n",
    "        if ifshuffle:\n",
    "            random.Random().shuffle(all_frame_id_list)\n",
    "        data_index = data_index[:, all_frame_id_list]\n",
    "        \n",
    "        if setname == 'train':\n",
    "            data_index = np.append(data_index, data_index[:, :self.args.batch_size], 1)\n",
    "            \n",
    "        return data_index\n",
    "    \n",
    "    # MODIFIED FUNCTION\n",
    "    def get_seq_from_index_balance(self, frameped_dict, pedtraject_dict, data_index, setname):\n",
    "        '''\n",
    "        Query the trajectories fragments from data sampling index.\n",
    "        Notes: Divide the scene if there are too many people; accumulate the scene if there are few people.\n",
    "               This function takes less gpu memory.\n",
    "        '''\n",
    "        batch_data_mass = []\n",
    "        batch_data = []\n",
    "        Batch_id = []\n",
    "\n",
    "        temp = self.args.batch_around_ped \n",
    "        if setname == 'train':\n",
    "            skip = self.trainskip\n",
    "        else:\n",
    "            skip = self.testskip\n",
    "        \n",
    "        ped_cnt = 0\n",
    "        last_frame = 0\n",
    "        \n",
    "        for i in range(data_index.shape[1]):\n",
    "            if i % 100 == 0:\n",
    "                print(i, '/', data_index.shape[1])\n",
    "            cur_frame, cur_set, _ = data_index[:, i]\n",
    "            framestart_pedi = set(frameped_dict[cur_set][cur_frame])\n",
    "            try:\n",
    "                frameend_pedi = set(frameped_dict[cur_set][cur_frame + self.args.seq_length * skip])\n",
    "            except:\n",
    "                continue\n",
    "            present_pedi = framestart_pedi | frameend_pedi\n",
    "            \n",
    "            if (framestart_pedi & frameend_pedi).__len__() == 0:\n",
    "                continue\n",
    "            traject = ()\n",
    "            IFfull = []\n",
    "            \n",
    "            for ped in present_pedi:\n",
    "                # if there are missing frames in traj, skip the vehicle, as it causes errors in find_trajectory_fragment\n",
    "                if (pedtraject_dict[cur_set][ped][-1, 0] - pedtraject_dict[cur_set][ped][0, 0]) \\\n",
    "                    != (pedtraject_dict[cur_set][ped].shape[0]-1)*skip :\n",
    "                    continue\n",
    "                \n",
    "                cur_trajec, iffull, ifexistobs = self.find_trajectory_fragment(pedtraject_dict[cur_set][ped],\n",
    "                                                                               cur_frame, self.args.seq_length, skip)\n",
    "                if len(cur_trajec) == 0: \n",
    "                    continue\n",
    "                if ifexistobs == False:\n",
    "                    # Just ignore trajectories if their data don't exsist at the last obversed time step (easy for data shift)\n",
    "                    continue\n",
    "                if sum(cur_trajec[:, 0] > 0) < 5:\n",
    "                    # filter trajectories have too few frame data\n",
    "                    continue\n",
    "\n",
    "                cur_trajec = (cur_trajec[:, 1:].reshape(-1, 1, 2),)\n",
    "                traject = traject.__add__(cur_trajec)\n",
    "                IFfull.append(iffull)\n",
    "                \n",
    "            if traject.__len__() < 1:\n",
    "                continue\n",
    "            if sum(IFfull) < 1:\n",
    "                continue\n",
    "            traject_batch = np.concatenate(traject, 1)\n",
    "            batch_pednum = sum([i.shape[1] for i in batch_data]) + traject_batch.shape[1] \n",
    "\n",
    "            cur_pednum = traject_batch.shape[1]\n",
    "            ped_cnt += cur_pednum\n",
    "            batch_id = (cur_set, cur_frame,)\n",
    "            \n",
    "            if cur_pednum >= self.args.batch_around_ped * 2:\n",
    "                # too many people in current scene\n",
    "                # split the scene into two batches\n",
    "                ind = traject_batch[self.args.obs_length - 1].argsort(0)\n",
    "                cur_batch_data, cur_Batch_id = [], []\n",
    "                Seq_batchs = [traject_batch[:, ind[:cur_pednum // 2, 0]], traject_batch[:, ind[cur_pednum // 2:, 0]]]\n",
    "                for sb in Seq_batchs:\n",
    "                    cur_batch_data.append(sb)\n",
    "                    cur_Batch_id.append(batch_id)\n",
    "                    cur_batch_data = self.massup_batch(cur_batch_data)\n",
    "                    batch_data_mass.append((cur_batch_data, cur_Batch_id,))\n",
    "                    cur_batch_data = []\n",
    "                    cur_Batch_id = []\n",
    "                    \n",
    "                last_frame = i\n",
    "            elif cur_pednum >= self.args.batch_around_ped:\n",
    "                # good pedestrian numbers\n",
    "                cur_batch_data, cur_Batch_id = [], []\n",
    "                cur_batch_data.append(traject_batch)\n",
    "                cur_Batch_id.append(batch_id)\n",
    "                cur_batch_data = self.massup_batch(cur_batch_data)\n",
    "                batch_data_mass.append((cur_batch_data, cur_Batch_id,))\n",
    "                \n",
    "                last_frame = i\n",
    "            else:  # less pedestrian numbers <64\n",
    "                # accumulate multiple framedata into a batch\n",
    "                if batch_pednum > self.args.batch_around_ped:\n",
    "                    # enough (accumulated) people in the scene\n",
    "                    batch_data.append(traject_batch)\n",
    "                    Batch_id.append(batch_id)\n",
    "\n",
    "                    batch_data = self.massup_batch(batch_data) #!!\n",
    "                    batch_data_mass.append((batch_data, Batch_id,))\n",
    "\n",
    "                    last_frame = i\n",
    "                    batch_data = []\n",
    "                    Batch_id = []\n",
    "                else:\n",
    "                    batch_data.append(traject_batch)\n",
    "                    Batch_id.append(batch_id)\n",
    "                \n",
    "\n",
    "        if last_frame < data_index.shape[1] - 1 and setname == 'test' and batch_pednum > 1:\n",
    "            batch_data = self.massup_batch(batch_data) #!!\n",
    "            batch_data_mass.append((batch_data, Batch_id,))\n",
    "        self.args.batch_around_ped = temp\n",
    "        \n",
    "        return batch_data_mass\n",
    "    \n",
    "    def find_trajectory_fragment(self, trajectory, startframe, seq_length, skip):\n",
    "        '''\n",
    "        Query the trajectory fragment based on the index. Replace where data doesn't exsist with 0.\n",
    "        '''\n",
    "        return_trajec = np.zeros((seq_length, 3))\n",
    "        endframe = startframe + (seq_length) * skip\n",
    "        start_n = np.where(trajectory[:, 0] == startframe)\n",
    "        end_n = np.where(trajectory[:, 0] == endframe)\n",
    "        iffull = False\n",
    "        ifexsitobs = False\n",
    "\n",
    "        if start_n[0].shape[0] == 0 and end_n[0].shape[0] != 0:\n",
    "            start_n = 0\n",
    "            end_n = end_n[0][0]\n",
    "            if end_n == 0:\n",
    "                return return_trajec, iffull, ifexsitobs\n",
    "            \n",
    "        elif end_n[0].shape[0] == 0 and start_n[0].shape[0] != 0:\n",
    "            start_n = start_n[0][0]\n",
    "            end_n = trajectory.shape[0]\n",
    "\n",
    "        elif end_n[0].shape[0] == 0 and start_n[0].shape[0] == 0:\n",
    "            start_n = 0\n",
    "            end_n = trajectory.shape[0]\n",
    "\n",
    "        else:\n",
    "            end_n = end_n[0][0]\n",
    "            start_n = start_n[0][0]\n",
    "\n",
    "        candidate_seq = trajectory[start_n:end_n]\n",
    "        offset_start = int((candidate_seq[0, 0] - startframe) // skip)\n",
    "\n",
    "        offset_end = self.args.seq_length + int((candidate_seq[-1, 0] - endframe) // skip)\n",
    "                \n",
    "        return_trajec[offset_start:offset_end + 1, :3] = candidate_seq\n",
    "        \n",
    "        if return_trajec[self.args.obs_length - 1, 1] != 0:\n",
    "            ifexsitobs = True\n",
    "\n",
    "        if offset_end - offset_start >= seq_length - 1:\n",
    "            iffull = True\n",
    "\n",
    "        return return_trajec, iffull, ifexsitobs\n",
    "    \n",
    "    def massup_batch(self, batch_data):\n",
    "        '''\n",
    "        Massed up data fragements in different time window together to a batch\n",
    "        '''\n",
    "        num_Peds = 0\n",
    "        for batch in batch_data:\n",
    "            num_Peds += batch.shape[1]\n",
    "\n",
    "        seq_list_b = np.zeros((self.args.seq_length, 0))\n",
    "        nodes_batch_b = np.zeros((self.args.seq_length, 0, 2))\n",
    "        nei_list_b = np.zeros((self.args.seq_length, num_Peds, num_Peds))\n",
    "        nei_num_b = np.zeros((self.args.seq_length, num_Peds))\n",
    "        num_Ped_h = 0\n",
    "        batch_pednum = []\n",
    "        for batch in batch_data:\n",
    "            num_Ped = batch.shape[1]\n",
    "            seq_list, nei_list, nei_num = self.get_social_inputs_numpy(batch)\n",
    "            nodes_batch_b = np.append(nodes_batch_b, batch, 1)\n",
    "            seq_list_b = np.append(seq_list_b, seq_list, 1)\n",
    "            nei_list_b[:, num_Ped_h:num_Ped_h + num_Ped, num_Ped_h:num_Ped_h + num_Ped] = nei_list\n",
    "            nei_num_b[:, num_Ped_h:num_Ped_h + num_Ped] = nei_num\n",
    "            batch_pednum.append(num_Ped)\n",
    "            num_Ped_h += num_Ped\n",
    "            \n",
    "        return (nodes_batch_b, seq_list_b, nei_list_b, nei_num_b, batch_pednum)\n",
    "    \n",
    "    def get_social_inputs_numpy(self, inputnodes):\n",
    "        '''\n",
    "        Get the sequence list (denoting where data exsist) and neighboring list (denoting where neighbors exsist).\n",
    "        '''\n",
    "        num_Peds = inputnodes.shape[1]\n",
    "\n",
    "        seq_list = np.zeros((inputnodes.shape[0], num_Peds))\n",
    "        # denote where data not missing\n",
    "\n",
    "        for pedi in range(num_Peds):\n",
    "            seq = inputnodes[:, pedi]\n",
    "            seq_list[seq[:, 0] != 0, pedi] = 1\n",
    "\n",
    "        # get relative cords, neighbor id list\n",
    "        nei_list = np.zeros((inputnodes.shape[0], num_Peds, num_Peds))\n",
    "        nei_num = np.zeros((inputnodes.shape[0], num_Peds))\n",
    "\n",
    "        # nei_list[f,i,j] denote if j is i's neighbors in frame f\n",
    "        for pedi in range(num_Peds):\n",
    "            nei_list[:, pedi, :] = seq_list\n",
    "            nei_list[:, pedi, pedi] = 0\n",
    "            nei_num[:, pedi] = np.sum(nei_list[:, pedi, :], 1)\n",
    "            seqi = inputnodes[:, pedi]\n",
    "            for pedj in range(num_Peds):\n",
    "                seqj = inputnodes[:, pedj]\n",
    "                select = (seq_list[:, pedi] > 0) & (seq_list[:, pedj] > 0)\n",
    "\n",
    "                relative_cord = seqi[select, :2] - seqj[select, :2]\n",
    "\n",
    "                # invalid data index\n",
    "                select_dist = (abs(relative_cord[:, 0]) > self.args.neighbor_thred) | (\n",
    "                        abs(relative_cord[:, 1]) > self.args.neighbor_thred)\n",
    "\n",
    "                nei_num[select, pedi] -= select_dist\n",
    "\n",
    "                select[select == True] = select_dist\n",
    "                nei_list[select, pedi, pedj] = 0\n",
    "        return seq_list, nei_list, nei_num\n",
    "    \n",
    "    def get_train_batch(self, idx):\n",
    "        batch_data, batch_id = self.trainbatch[idx]\n",
    "\n",
    "        return batch_data, batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = processor(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pneuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2914486fd10fe69a4eb38d31c4c20b960e431fefe4c64bccdeb4d2814b0bc480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
